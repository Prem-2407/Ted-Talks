{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prem-2407/Ted-Talks/blob/main/Almabetter_project_Ted_Talks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HANanpl4JWfB"
      },
      "source": [
        "# Project Title:  Ted Talks Views Prediction\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPdXUjMIJpkH"
      },
      "source": [
        "TED is decoted to spereading powerful ideas on just about any topic . These datasets contain over 4,000 talks including transcripts in many languages.\n",
        "\n",
        "Founded in 1984 by Richard Salman as a nonprofit organization that aimed at bringing experts fromm the fileds of Technology ,Entertainmnet and Design together ..\n",
        "\n",
        "TED conferences have gone on to becomme theMecca of ideas from virtually all walks of life .As of 2015 ,TED and its sister TEDx chapters have published more than 2000 talks for free consumption by the masses  and its speaker list boasts of the likes of AI Gore, Jimmy Wales,ShahRukh Khan,Bll Gates.\n",
        "\n",
        "\n",
        "Our main objective id to build a predictive model to predict the number of views for the respective videos from the TEDx website.As we have a continous varible to predict ,we use regression models to predict the views of the TED talks ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6t5fYFiL86r"
      },
      "source": [
        "#**Steps to follow** \n",
        "\n",
        "**1) Data Loading**\n",
        "\n",
        "**2)Data Cleaning**\n",
        "\n",
        "**3)EDA**\n",
        "\n",
        "**4) Feature engineering**\n",
        "\n",
        "**5) Model building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHOJEgrUpz4f"
      },
      "outputs": [],
      "source": [
        "  # Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK1JjW_9M-ZP"
      },
      "source": [
        "#**Mounting The Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRH10WfxqrUq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctnqHHG7NGH1"
      },
      "source": [
        "#**Loading the Data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38IswYEHr_QA"
      },
      "outputs": [],
      "source": [
        "tedtalk_df=pd.read_csv('//content/drive/MyDrive/Alma Better Ted Talks/data_ted_talks.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNRfef1tsQ6v"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6alaf9xsWFo"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eChtEA7MsZVC"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1uZK95YNUq_"
      },
      "source": [
        "**Shape of the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrN6BVegscH_"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amn6Jk0VNd4o"
      },
      "source": [
        "By the shape we can get that there are **4005 Rows** Along with **19 variables** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGxj0drN0Ae"
      },
      "source": [
        "#**Data Cleaning** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGCJyQTN8Ay"
      },
      "source": [
        "**Missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjelQt29spEn"
      },
      "outputs": [],
      "source": [
        "def missing_values_table(df):\n",
        "        mis_val =df.isna().sum()\n",
        "        mis_val_percent = 100 *df.isna().sum() / len(df)\n",
        "        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "        mz_table = mz_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        mz_table['Data Type'] = df.dtypes\n",
        "        mz_table = mz_table.sort_values('% of Total Values', ascending=False).round(1)\n",
        "        print (\" selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]),\"\\n\\n\")\n",
        "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
        "        return mz_table\n",
        "\n",
        "missing_values_table(tedtalk_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff3hsjHkss_N"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(tedtalk_df.isnull(),cbar=False,yticklabels=False,cmap='OrRd');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_TJG5HvOEWV"
      },
      "source": [
        "We can see that we have lotof missing values in several columns ,Therefore we will be treated after the EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biTh_gSSOYT2"
      },
      "source": [
        "Here many columns have the data in the form of a string and dictionaries .To acess the data and to build model,we sould have cleaned data which can be easily acessed .So we are converting all colummns to its proper datatype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6oVgvBpsy5h"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['occupations'].fillna(str({0:['other']}),inplace=True)\n",
        "#filling NA with other\n",
        "\n",
        "tedtalk_df['occupations'] = tedtalk_df.apply(lambda row: eval(row['occupations']), axis=1)\n",
        "#to convert from str to dict\n",
        "\n",
        "tedtalk_df['occupations'] = tedtalk_df['occupations'].apply(lambda x: x.get(0))\n",
        "#to extract the dict values\n",
        "\n",
        "tedtalk_df['about_speakers'].fillna(str({0:'Not specified'}),inplace=True)\n",
        "#filling the NA's with 'Not specified'\n",
        "\n",
        "tedtalk_df['about_speakers'] = tedtalk_df.apply(lambda row: eval(row['about_speakers']), axis=1)\n",
        "#to convert from str to dict\n",
        "\n",
        "tedtalk_df['about_speakers'] = tedtalk_df['about_speakers'].apply(lambda x: x.get(0))\n",
        "#to extract the dict values\n",
        "\n",
        "tedtalk_df['recorded_date'].fillna(tedtalk_df['recorded_date'].mode()[0],inplace=True)\n",
        "#since dates are kind of discrete vaariables, we replace NA with mode.\n",
        "\n",
        "tedtalk_df['available_lang'] = tedtalk_df['available_lang'].apply(lambda row: eval(row))\n",
        "tedtalk_df['topics'] = tedtalk_df['topics'].apply(lambda row: eval(row))\n",
        "tedtalk_df['related_talks'] = tedtalk_df['related_talks'].apply(lambda row: eval(row))\n",
        "#converting all the remaining columns which were in strings to their respective datatypes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmNLJoHoPgFI"
      },
      "source": [
        "Here ,some of the mmissing values are treated which missinng values % was very less.its done becaues it will give better visualizations in our EDA ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfBmLfbQs1pa"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.drop(['talk_id','all_speakers','url'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIFrVgQ5Pihg"
      },
      "source": [
        "These columns are deleted as its not carrying much significance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te1eXoZrs3yA"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16UWXEYlPvXd"
      },
      "source": [
        "Date ,Month and year columns are converted to its proper Datetimme datatype."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXQamqt4s8Tk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "tedtalk_df['published_date']=pd.to_datetime(tedtalk_df['published_date'], format='%Y-%m-%d')\n",
        "tedtalk_df['recorded_date']=pd.to_datetime(tedtalk_df['recorded_date'], format='%Y-%m-%d')\n",
        "#converting the published and recorded date as datetime datatype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DMojwdcs-1X"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['release_day'] = tedtalk_df['published_date'].apply(lambda x: x.weekday())\n",
        "tedtalk_df['release_month']=tedtalk_df['published_date'].apply(lambda x: x.month)\n",
        "tedtalk_df['release_year'] = tedtalk_df['published_date'].apply(lambda x: x.year)\n",
        "#finding out the respected day,month and year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOPksHG7tEgy"
      },
      "outputs": [],
      "source": [
        "week_day={0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
        "tedtalk_df['release_day']=tedtalk_df['release_day'].map(week_day)\n",
        "#tedtalk_day was interms of numbers which were holding the record of monday till sunday...so converting that to exact days here\n",
        "\n",
        "month_dict={1:'Jan',2:'Feb',3:'March',4:'April',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
        "tedtalk_df['release_month']=tedtalk_df['release_month'].map(month_dict)\n",
        "#again converting the month which was in numbers to exact month names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRhsiPFPtG8t"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFREtelctJZV"
      },
      "outputs": [],
      "source": [
        "ted_talk_df = tedtalk_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yoNBpMCtL0m"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSGT-iL1QEem"
      },
      "source": [
        "Now that a basic level cleaning of dataet is done we will start with the processs of EDA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxNuOI0QPJ7"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0WKS4zmQRqg"
      },
      "source": [
        "# Speaker_1_column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXj_kSTsQZm3"
      },
      "source": [
        "**Speakers with top 20 total views wrt their numbero f talks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urIiE5W4tPRu"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['speaker_1'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEvn1T2vtS_0"
      },
      "outputs": [],
      "source": [
        "speaker_with_most_talks = pd.DataFrame(tedtalk_df['speaker_1'].value_counts()).reset_index()\n",
        "speaker_with_most_talks.rename(columns={'index':'speaker_1', 'speaker_1':'num_of_talks'}, inplace=True)\n",
        "\n",
        "\n",
        "speaker_total_views_df=tedtalk_df.groupby('speaker_1').agg({'views' : 'sum'}).reset_index()\n",
        "#speaker with respected total views\n",
        "speaker_tv_df = speaker_total_views_df.head(20).sort_values('views',ascending=False)\n",
        "\n",
        "speaker_report = speaker_with_most_talks.merge(speaker_total_views_df,on='speaker_1').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkHI5a5VtWJg"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "speaker_numeric = speaker_report[['num_of_talks','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(speaker_numeric)\n",
        "#print(scaled)\n",
        "speaker_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "speaker_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))\n",
        "speaker_report = speaker_report.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgHS4qeJtV7n"
      },
      "outputs": [],
      "source": [
        "# graph for speakers which has maximum talks with respect to views\n",
        "\n",
        "# # plotting the graph\n",
        "\n",
        "speaker_report_1 = speaker_report.sort_values('num_of_talks_scaler')\n",
        "speaker_report_2 = speaker_report.sort_values('views')\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "#plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='speaker_1',y='num_of_talks_scaler',data = speaker_report_1)\n",
        "sns.lineplot(x='speaker_1',y='views_scaler',data=speaker_report_1,marker='o')\n",
        "plt.title('Speaker with top 10 number of talks and total views')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='speaker_1',y='num_of_talks_scaler',data = speaker_report_2)\n",
        "sns.lineplot(x='speaker_1',y='views_scaler',data=speaker_report_2,marker='o')\n",
        "plt.title('Speaker with top 10 total views and number_of_talks')\n",
        "#speaker_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQogV4vTQqXO"
      },
      "source": [
        "We have the visualization for the same data from two different aspect here.\n",
        "\n",
        "1) First is the graph which contains speakerswho has given most numbers of talks and the line graph represents their views.We can observe that Bill Gates was the influneced  person. So even if his number of taks are less, he has received more number of views .So the speaker influences the number of views of the Talks.We can see somme hike in the graph for the influneced speakers.\n",
        " \n",
        " 2) second is the graph which is stored according to the number odf vviews talks received .We van obser e that the number of talks is not directly contributing to the number of views talks can receive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhd9Py9SKff"
      },
      "source": [
        "#**Speaker who received more number of views for one video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STqMjptKtgB-"
      },
      "outputs": [],
      "source": [
        "most_popular_video_df = tedtalk_df.nlargest(10,['views'])\n",
        "most_popular_video_df[['speaker_1','views','title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvwg6aPdtjDQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.title(\"most views received for the single video by speaker\")\n",
        "sns.barplot(x='speaker_1',y='views',data=most_popular_video_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B5iJk4ySWxk"
      },
      "source": [
        "This graph represents the maximum views a single video recived .Sir Ken Robinson is the ione whi recived maxximmum number for a single video "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3o06UI8S59m"
      },
      "source": [
        "#**Occupations column**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMZBAdLTBsi"
      },
      "source": [
        "**Top occupations of the speaker**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU0qTl11tobx"
      },
      "outputs": [],
      "source": [
        "\n",
        "tedtalk_df['speaker_occupation']=pd.Series()\n",
        "for i in range(len(tedtalk_df)):\n",
        "  tedtalk_df.loc[i,'speaker_occupation'] = tedtalk_df['occupations'][i][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C-baKXptqZY"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['speaker_occupation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FaM8yR_ttNe"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.drop('occupations',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypSCMp80tvwn"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['speaker_occupation'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMxtcGHfTP6W"
      },
      "source": [
        "By this we can observe that there are 1552 different occupations for the respective speakers in TED talk videos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhyfPtdqtvqO"
      },
      "outputs": [],
      "source": [
        "speaker_occupations_df = pd.DataFrame(tedtalk_df['speaker_occupation'].value_counts()).reset_index()\n",
        "speaker_occupations_df.rename(columns={'index':'Occupations', 'speaker_occupation':'Number_of_speakers'}, inplace=True)\n",
        "\n",
        "occupations_with_most_views = tedtalk_df.groupby(['speaker_occupation'],dropna=True)['views'].mean().reset_index()\n",
        "occupations_with_most_views.rename(columns={'speaker_occupation':'Occupations', 'views':'views'}, inplace=True)\n",
        "\n",
        "occupation_report = speaker_occupations_df.merge(occupations_with_most_views,on='Occupations')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVSqUVd2t3FK"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "occupation_numeric = occupation_report[['Number_of_speakers','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(occupation_numeric)\n",
        "#print(scaled)\n",
        "occupation_report ['Number_of_speakers_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "occupation_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))\n",
        "occupation_report = occupation_report.loc[1:21,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brfG-6iqt7LC"
      },
      "outputs": [],
      "source": [
        "# graph for occupation which has maximum talks with respect to views\n",
        "\n",
        "# # plotting the graph\n",
        "\n",
        "occupation_report_1 = occupation_report.sort_values('Number_of_speakers')\n",
        "occupation_report_2 = occupation_report.sort_values('views')\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "#plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='Occupations',y='Number_of_speakers_scaler',data = occupation_report_1)\n",
        "sns.lineplot(x='Occupations',y='views_scaler',data = occupation_report_1,marker='o')\n",
        "plt.title('Occupations with number of speakers and respected average views')\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "#plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='Occupations',y='Number_of_speakers_scaler',data = occupation_report_2)\n",
        "sns.lineplot(x='Occupations',y='views_scaler',data = occupation_report_2,marker='o')\n",
        "plt.title('Occupations with average views and respected number of speakers')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhUyZvIgTgal"
      },
      "source": [
        "\n",
        "Again we have 2 representation of the same graphs, once sorted according to the number of talks then according to the number of views.\n",
        "\n",
        "(1) In first graph, we can see that, again number of talks for the respective occupations are not directly contributing to the number of views. Instead the type of occupations are bringinfg a hike in number of views. For example, psychology (which everybody loves to listen about) and Activist (Who will make everybody listen to them by theri words). The occupations (might be their skills to talk to) does matter in number of views.\n",
        "\n",
        "(2) Second graph contains the grapph which is sorted according to the number of views. We can See that the top 3 views were received by Psychologist, Activist and Author who all are very good in making people love what they speak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkorTu7iTjLl"
      },
      "source": [
        "#**Occupations which received maximum number of views for a single video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNpk3zk9t_Of"
      },
      "outputs": [],
      "source": [
        "\n",
        "most_popular_video_df = tedtalk_df.nlargest(10,['views'])\n",
        "most_popular_video_df[['speaker_occupation','views','title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VPHcyvBuCgr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.title(\"most viewd video_Occupation\")\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='speaker_occupation',y='views',data=most_popular_video_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AAki32xT4NT"
      },
      "source": [
        "we can observe that the maximum number of views for a single video was received by the occuppations Author followed by Psychologist ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E95wTChUUQtL"
      },
      "source": [
        "#**Published date and released date**\n",
        "\n",
        "**Findiing out on which day most of the vidoes were released**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk8hoq8UuIiH"
      },
      "outputs": [],
      "source": [
        "#creating a df which holds the number of talks on each day\n",
        "talk_rel_day_df = pd.DataFrame(tedtalk_df['release_day'].value_counts()).reset_index()\n",
        "talk_rel_day_df.rename(columns={'index':'release_day', 'release_day':'num_of_talks'}, inplace=True)\n",
        "\n",
        "#creating another df which holds the record of views on each day\n",
        "popular_day = tedtalk_df[['release_day','views']].groupby('release_day').agg({'views' : 'mean'}).sort_values('views',ascending=False).reset_index()\n",
        "\n",
        "#merging both df\n",
        "talk_day_report = talk_rel_day_df.merge(popular_day,on='release_day')\n",
        "\n",
        "talk_day_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTsigoiSuMPY"
      },
      "outputs": [],
      "source": [
        "#Sorting the days by using another dataframe with the day values\n",
        "\n",
        "day_value = pd.DataFrame([['Sunday',0],['Monday',1],['Tuesday',2],['Wednesday',3],['Thursday',4],['Friday',5],['Saturday',6]])\n",
        "day_value.rename(columns={0:'release_day',1:'Day_value'},inplace=True)\n",
        "day_value\n",
        "talk_day_report = talk_day_report.merge(day_value,on='release_day')\n",
        "\n",
        "talk_day_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td08MrQ4uPrP"
      },
      "outputs": [],
      "source": [
        "talk_day_report = talk_day_report.sort_values('Day_value',ascending=True)\n",
        "talk_day_report = talk_day_report.drop('Day_value',axis=True)\n",
        "talk_day_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npsIvg5luT87"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "talk_day_numeric = talk_day_report[['num_of_talks','views']]\n",
        "print(talk_day_numeric)\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(talk_day_numeric)\n",
        "#print(scaled)\n",
        "talk_day_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "talk_day_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ohWl5cQuXTf"
      },
      "outputs": [],
      "source": [
        "talk_day_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2MKNQkuadf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "for i in talk_day_report.iloc[:,3:]:\n",
        "  sns.lineplot(data=talk_day_report,x='release_day',y=i,marker='o')\n",
        "\n",
        "plt.legend(labels=talk_day_numeric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSGE9uKUui9"
      },
      "source": [
        "\n",
        "We can observe in this graph about the comparision of number of talks and as well as the number of views a particular day received. We can see that on saturday the number of talks and as well as the number of views were low. And on Sunday, Wednesday and Friday the release were high. But the number of views were high only on Sunday.\n",
        "\n",
        "It may seem that only on weekends people were interested to give a time for TED talk videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFXjuuKVUzAP"
      },
      "source": [
        "#**Month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Dkd20eudw_"
      },
      "outputs": [],
      "source": [
        "! pip install sorted-months-weekdays\n",
        "! pip install sort-dataframeby-monthorweek\n",
        "\n",
        "from sorted_months_weekdays import *\n",
        "from sort_dataframeby_monthorweek import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fYtp1NjuyUo"
      },
      "outputs": [],
      "source": [
        "# creating data frame for num of talks in a particular month \n",
        "talk_rel_month_df = pd.DataFrame(tedtalk_df['release_month'].value_counts()).reset_index()\n",
        "talk_rel_month_df.rename(columns={'index':'release_month', 'release_month':'num_of_talks'}, inplace=True)\n",
        "\n",
        "#Printing the most popular release month according to average Views\n",
        "popular_month = tedtalk_df[['release_month','views']].groupby('release_month').agg({'views' : 'mean'}).sort_values('release_month').reset_index()\n",
        "\n",
        "#merging two dataframes which has number of talks and views received in the same df\n",
        "talk_month_report = talk_rel_month_df.merge(popular_month,on='release_month')\n",
        "\n",
        "#sorting according to month in here\n",
        "talk_month_report = Sort_Dataframeby_Month(df=talk_month_report ,monthcolumnname='release_month')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAOvfEENu5E2"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "talk_month_numeric = talk_month_report[['num_of_talks','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(talk_month_numeric)\n",
        "#print(scaled)\n",
        "talk_month_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "talk_month_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evFhfr2Du7_F"
      },
      "outputs": [],
      "source": [
        "\n",
        "talk_month_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3lVvT3u_U_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "for i in talk_month_report.iloc[:,3:]:\n",
        "  sns.lineplot(data=talk_month_report,x='release_month',y=i,marker='o')\n",
        "\n",
        "plt.legend(talk_month_numeric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhmTTWcU5QR"
      },
      "source": [
        "\n",
        "Here we can observe that it has the comparision wise line graph for number of talks and number of views for the respective month.\n",
        "\n",
        "Most number of videos wwere released on the month of April where the lease was released on the month of August. But the number of views were maximum for the videos which were released on the minth of March and it was lease for the videos of Novemb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1tjStNEU-dD"
      },
      "source": [
        "#**YEAR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkDfVmsCvOiN"
      },
      "outputs": [],
      "source": [
        "talk_rel_year_df = pd.DataFrame(tedtalk_df['release_year'].value_counts()).reset_index()\n",
        "talk_rel_year_df.rename(columns={'index':'release_year', 'release_year':'num_of_talks'}, inplace=True)\n",
        "\n",
        "popular_year = tedtalk_df[['release_year','views']].groupby('release_year').agg({'views' : 'mean'}).sort_values('release_year',ascending=False).reset_index()\n",
        "\n",
        "talk_year_report = talk_rel_year_df.merge(popular_year,on='release_year').sort_values('release_year')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ta4UKgEvTfM"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "talk_year_numeric = talk_year_report[['num_of_talks','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(talk_year_numeric)\n",
        "#print(scaled)\n",
        "talk_year_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "talk_year_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMKunbOuvV6S"
      },
      "outputs": [],
      "source": [
        "talk_year_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bag5TsbKvYvT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "for i in talk_year_report.iloc[:,3:]:\n",
        "  sns.lineplot(data=talk_year_report,x='release_year',y=i,marker='o')\n",
        "plt.legend(labels=talk_year_numeric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoB2bdtXVDCA"
      },
      "source": [
        "We can observe the line graph for almost 20 years of number of talks and number of views. We can conclude that Number of talks were morein old days but eventually it was reduced hit the minimum in 2019. It hits the hike in 2007 though. But the number of views were gradually increasing from year to year with a little bit of ups and downs. It was high during the 2019 when the number of talks were minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYfe0bvVGCJ"
      },
      "source": [
        "**To view the number of views recieved per day on monthly basis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVc70Z0Fvcse"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "graph_df = tedtalk_df.copy()\n",
        "g = sns.FacetGrid(graph_df, col='release_day', height=8, aspect=.5)\n",
        "td = pd.to_datetime(date.today(), format='%Y-%m-%d')\n",
        "graph_df['video_age'] = (td - tedtalk_df['published_date']).apply(lambda x: x.days)\n",
        "graph_df['views_per_day'] = graph_df['views'] / ( graph_df['video_age'] + 1 )\n",
        "g.map(sns.barplot, 'release_month', 'views_per_day')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxCVgBJRVVIq"
      },
      "source": [
        "**Comment and duration columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hFv7CmvfqJ"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(data=tedtalk_df,x='comments', y='views')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1JaNDkqviBD"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(data=tedtalk_df,x='duration', y='views')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLlp_75rvk5l"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "sns.distplot(tedtalk_df['views'])\n",
        "plt.subplot(1,3,2)\n",
        "sns.distplot(tedtalk_df['comments'])\n",
        "plt.subplot(1,3,3)\n",
        "sns.distplot(tedtalk_df['duration'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGUkPDZVeD0"
      },
      "source": [
        "we can observe the data distribution of the numerical columns views ,comments and duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVirEkSQVmsc"
      },
      "source": [
        "#**Events with respect to talk and views**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWPicFCVvqqI"
      },
      "outputs": [],
      "source": [
        "#count of events\n",
        "\n",
        "event_count_df = pd.DataFrame(tedtalk_df['event'].value_counts()).reset_index()\n",
        "event_count_df.rename(columns={'index':'event', 'event':'num_of_talks'}, inplace=True)\n",
        "\n",
        "#event with max views\n",
        "popular_event=tedtalk_df[['event','views']].groupby('event').agg({'views' : 'mean'}).sort_values('views',ascending=False).reset_index()\n",
        "\n",
        "event_report = event_count_df.merge(popular_event,on='event')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-wtTBHUvt02"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "event_numeric = event_report[['num_of_talks','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(event_numeric)\n",
        "#print(scaled)\n",
        "event_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "event_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8we8MOLvxSP"
      },
      "outputs": [],
      "source": [
        "# graph for event which has maximum talks with respect to views\n",
        "event_talk_views_report_1 = event_report.sort_values('num_of_talks',ascending=False).head(20)\n",
        "event_talk_views_report_2 = event_report.sort_values('views',ascending=False).head(20)\n",
        "# # plotting the graph\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='event',y='num_of_talks_scaler',data = event_talk_views_report_1)\n",
        "sns.lineplot(x='event',y='views_scaler',data=event_talk_views_report_1,marker='o')\n",
        "plt.title('Events with top 10 number of talks followed by their views')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='event',y='num_of_talks_scaler',data = event_talk_views_report_2)\n",
        "sns.lineplot(x='event',y='views_scaler',data=event_talk_views_report_2,marker='o')\n",
        "plt.title('Events with top 10 views followed by their number of talks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ojRb7hVwcy"
      },
      "source": [
        "\n",
        "This graph represents the number of views for the different views along with the number of talks. We can observe that the count of number of talks of event did not really matter for views. For top 10 events in the first graph, the number of views is almost the same. And in the second graph, for the top 10 nummber of views, event's number of talks were very low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qQ4u5Gqvz2s"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(event_report['event'],event_report['views'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaCPEPMgV0ok"
      },
      "source": [
        "This explains about the diffrent events with respect to the nummer of views .And we can see some hikes in number of views for very few events in here ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiIVIPiIWQUo"
      },
      "source": [
        "#**Native lang column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbtfmGHXv6E9"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['native_lang'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMj0UjwNv88p"
      },
      "outputs": [],
      "source": [
        "native_lang_df = pd.DataFrame(tedtalk_df['native_lang'].value_counts()).reset_index()\n",
        "\n",
        "native_lang_df.rename(columns={'index':'native_lang', 'native_lang':'num_of_talks'}, inplace=True)\n",
        "\n",
        "#event with max views\n",
        "popular_lang=tedtalk_df[['native_lang','views']].groupby('native_lang').agg({'views' : 'mean'}).sort_values('views',ascending=False).reset_index()\n",
        "\n",
        "lang_report = native_lang_df.merge(popular_lang,on='native_lang')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boY598TuwBBJ"
      },
      "outputs": [],
      "source": [
        "#taking only numeric columns to do the minmaxscaling\n",
        "lang_numeric = event_report[['num_of_talks','views']]\n",
        "#using standardization as both numeric columns are in different scale\n",
        "#after words it will be easy for us to do the graphs\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(lang_numeric)\n",
        "#print(scaled)\n",
        "lang_report ['num_of_talks_scaler'] = pd.Series(scaled[i][0] for i in range(len(scaled)))\n",
        "lang_report ['views_scaler'] = pd.Series(scaled[i][1] for i in range(len(scaled)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQPPwOxswEg8"
      },
      "outputs": [],
      "source": [
        "# graph for langauges which has maximum talks with respect to views\n",
        "#lang_report.sort_values('views',ascending=False,inplace=True)\n",
        "# # plotting the graph\n",
        "lang_report_1 = lang_report.sort_values('num_of_talks',ascending=False)\n",
        "lang_report_2 = lang_report.sort_values('views',ascending=False)\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.barplot(x='native_lang',y='num_of_talks_scaler',data = lang_report_1)\n",
        "sns.lineplot(x='native_lang',y='views_scaler',data=lang_report_1,marker='o')\n",
        "plt.title('Native languages with high number of talks followed by its views')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.barplot(x='native_lang',y='num_of_talks_scaler',data = lang_report_2)\n",
        "sns.lineplot(x='native_lang',y='views_scaler',data=lang_report_2,marker='o')\n",
        "plt.title('Native languages with top views followed by thier num_of_talks')\n",
        "\n",
        "lang_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psa9caAUWYf3"
      },
      "source": [
        "\n",
        "We can see the different native languages avaliable, number of talks in those languages and number of views it received. In the first graph we can see the charts which are sorted according to the number of talks. Obviously english being the common langauge, has most numer of talks. But it did not ensure abou tthe number of views. The second graph contains the charts which are sorted according to number of views. And we can see that Portuguese has received maximum average views for its talks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCKWGkteWbI6"
      },
      "source": [
        "# Available lang column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6_xQoOvWvYX"
      },
      "source": [
        "**subtitles count for all videos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMyW6ZOMWlrj"
      },
      "source": [
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTVBlbkswKlm"
      },
      "outputs": [],
      "source": [
        "lang_dict = {}\n",
        "for i in tedtalk_df.to_dict('records'):\n",
        "   tmp = i['available_lang']\n",
        "   for j in tmp:\n",
        "    if j in lang_dict:\n",
        "      lang_dict[j] += 1\n",
        "    else:\n",
        "      lang_dict[j] = 1 \n",
        "lang_df = pd.DataFrame(lang_dict.values(),index=lang_dict.keys(),columns=['count']).head(20).reset_index().sort_values('count',ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0PdcC6TwNKL"
      },
      "outputs": [],
      "source": [
        "lang_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyPs9FxJwQD9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.title('% of Videos dubbed in different languages')\n",
        "ax = sns.barplot(x='index',y='count',data=lang_df)\n",
        "plt.xlabel(\"Languages\")\n",
        "plt.ylabel('Counts')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItX8juanW2bf"
      },
      "source": [
        "This graph gives the idea about how many languages were availble as subtitles for the videos ,and again English commes in top as its the common language all over the world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUlxDOX4XgkL"
      },
      "source": [
        "**Topic column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5UxcT1qwUFB"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['topics']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDMb6yjXwXIM"
      },
      "outputs": [],
      "source": [
        "topics_df = pd.DataFrame(tedtalk_df['topics'].explode().value_counts().reset_index()).iloc[:,0:]\n",
        "topics_df.rename(columns={'index':'Topics','topics':'Number_of_talks'},inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlGHueH_wZ7I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x='Topics',y='Number_of_talks',data=topics_df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws9bK_xQXm4p"
      },
      "source": [
        "we can see thet the most TED talks were about science and technology .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gM0XONlX2n3"
      },
      "source": [
        "**Wordcloud for topics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODXi_rzowd_V"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "text = \" \".join(topics_df['Topics'])\n",
        "word_cloud = WordCloud(max_words=100,collocations = False, background_color = 'white').generate(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pWRvlM7whFj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,7))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcD-luyPYNXO"
      },
      "source": [
        "**Description column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCgUrP-ywkur"
      },
      "outputs": [],
      "source": [
        "text = \" \".join(tedtalk_df['description'])\n",
        "word_cloud = WordCloud(max_words=100,collocations = False, background_color = 'black').generate(text)\n",
        "plt.figure(figsize=(16,7))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bimJeqd3YWLJ"
      },
      "source": [
        "**Title column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX2ddfiFwoFV"
      },
      "outputs": [],
      "source": [
        "text = \" \".join(tedtalk_df['title'])\n",
        "word_cloud = WordCloud(max_words=100,collocations = False, background_color = 'red').generate(text)\n",
        "plt.figure(figsize=(16,7))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFwpTQGeYe14"
      },
      "source": [
        "**Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtzA4X46wr-g"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(round(tedtalk_df.corr(),2),annot=True,cmap='Greens');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz95FgDHYla8"
      },
      "source": [
        "Trying to get a correlation matrix with visualization for the numeric columns.We can observe no 2 column share correlation more than 60%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ztJBbQY7yd"
      },
      "source": [
        "#**Missing values in comments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlSiMB-awus7"
      },
      "outputs": [],
      "source": [
        "td_talk_df_2 = tedtalk_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ed0oFS_wzy5"
      },
      "outputs": [],
      "source": [
        "sns.distplot(tedtalk_df.comments.dropna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kz1LMtaw2CQ"
      },
      "outputs": [],
      "source": [
        "numeric_df = tedtalk_df[['views','comments','duration']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuAIw6PpZDSA"
      },
      "source": [
        "As more than 3% of the data were missing in comments ,we tried using the KNN imouter to predict the missing values ion here ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lEoS6sLw5q7"
      },
      "outputs": [],
      "source": [
        "#KNN to find the missing values\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Doing dummy for categorical variables\n",
        "numeric_df = pd.get_dummies(numeric_df)\n",
        "\n",
        "# Defining scaler and imputer objects\n",
        "scaler = StandardScaler()\n",
        "imputer = KNNImputer()\n",
        "\n",
        "# Imputing missing values with KNN if any\n",
        "numeric_df['comments'] = imputer.fit_transform((numeric_df['comments'].values.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wvd6yX7w8BX"
      },
      "outputs": [],
      "source": [
        "sns.distplot(numeric_df['comments'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAs18gzLw_2r"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['comments']=numeric_df['comments']\n",
        "tedtalk_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPzqbwKkxCbo"
      },
      "outputs": [],
      "source": [
        "ted_talk_3_df = tedtalk_df.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_UfkuhCZYl5"
      },
      "source": [
        "# **Handling Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjgmt-BNxHTk"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.describe().T\n",
        "#we can see that we have only numerical columns in here.\n",
        "#talk_id is the unique number which should not be considered\n",
        "#views is the dependent variable which should not be considered\n",
        "# so we consider only comments and duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN5XkHbXxMFL"
      },
      "outputs": [],
      "source": [
        "continous = ['comments','duration']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiKqI5OdxOhf"
      },
      "outputs": [],
      "source": [
        "for var in continous:\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    fig = sns.boxplot(y=tedtalk_df[var])\n",
        "    fig.set_title('')\n",
        "    fig.set_ylabel(var)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    fig = sns.distplot(tedtalk_df[var])\n",
        "    fig.set_ylabel('views')\n",
        "    fig.set_xlabel(var)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ta7fKg2xRaS"
      },
      "outputs": [],
      "source": [
        " tedtalk_df[['comments','duration']].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "655Jvg2gxTzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "#comments\n",
        "#Finding IQR\n",
        "Q3, Q1 = np.percentile(tedtalk_df['comments'], [75 ,25])\n",
        "IQR = Q3 - Q1\n",
        "upper = Q3 +1.5*IQR\n",
        "lower = Q1 - 1.5*IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUH8I4uzxWEC"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.loc[tedtalk_df['comments']>upper,'comments'] = upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-H85PAPxX6Y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(tedtalk_df['comments'])\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.distplot(tedtalk_df['comments'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru6iBYtVxapp"
      },
      "outputs": [],
      "source": [
        "tedtalk_df[['comments','duration']].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMDiGydMxdFH"
      },
      "outputs": [],
      "source": [
        "#duration\n",
        "#Finding IQR\n",
        "Q3, Q1 = np.percentile(tedtalk_df['duration'], [75 ,25])\n",
        "IQR = Q3 - Q1\n",
        "upper = Q3 +1.5*IQR\n",
        "lower = Q1 - 1.5*IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHV0gU_XxfHA"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.loc[tedtalk_df['duration']>upper,'duration'] = upper\n",
        "#borderizing the outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDm8gM5Mxipx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(tedtalk_df['duration'])\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.distplot(tedtalk_df['duration'])\n",
        "# plt.figure(figsize=(15,6))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# sns.boxplot(tedtalk_df['duration'])\n",
        "# plt.subplot(1, 2, 2)\n",
        "# sns.distplot(tedtalk_df['duration'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1FsPNDAZolb"
      },
      "source": [
        "Here to treat outliers, we have used IQR method. Whatever the data were more than the Q3+1.5*IQR we tried equating it into upper range only. Only Comments and duration were treated this way.\n",
        "\n",
        "To treat outliers in views cl=olumn which was a dependent column we tried using the Z score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lolnI-cTxvBt"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(tedtalk_df['views'])\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.distplot(tedtalk_df['views'])\n",
        "\n",
        "\n",
        "tedtalk_df = tedtalk_df[((tedtalk_df['views'] - tedtalk_df['views'].mean()) / tedtalk_df['views'].std()).abs() < 3]\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(tedtalk_df['views'])\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.distplot(tedtalk_df['views'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POrLMiiJx0H3"
      },
      "outputs": [],
      "source": [
        "ted_talk_2_df = tedtalk_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW6qw0qfx3Dp"
      },
      "outputs": [],
      "source": [
        "tedtalk_df = tedtalk_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOvkmj3Ux5J-"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj_DyExaabvX"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF95ytp1x8a5"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "today = pd.to_datetime(date.today(), format='%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prqbii-uyG0e"
      },
      "outputs": [],
      "source": [
        "#creating a column with video age\n",
        "tedtalk_df['video_age'] = (today - tedtalk_df['published_date']).apply(lambda x: x.days)\n",
        "# Creating a daily views column\n",
        "tedtalk_df['views_per_day'] = tedtalk_df['views'] / ( tedtalk_df['video_age'] + 1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvgBhJPMyBgf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Finding average views according to first speaker\n",
        "tedtalk_df['speaker_1_average_views'] = tedtalk_df['speaker_1'].map(list(tedtalk_df.groupby('speaker_1').agg({'views_per_day' : 'mean'}).\n",
        "                                                         sort_values(['views_per_day'],ascending=False).to_dict().values())[0])\n",
        "\n",
        "#Finding average views according to event \n",
        "tedtalk_df['event_average_views'] = tedtalk_df['event'].map(list(tedtalk_df.groupby('event').agg({'views_per_day' : 'mean'}).\n",
        "                                                 sort_values(['views_per_day'],ascending=False).to_dict().values())[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm51U99VyT8L"
      },
      "outputs": [],
      "source": [
        "#Checking for unique topic\n",
        "unique_topics=[]\n",
        "for i in range(0,len(tedtalk_df)):\n",
        "  temp=tedtalk_df['topics'][i]\n",
        "  for i in temp:\n",
        "    if(i not in unique_topics):\n",
        "      unique_topics.append(i)\n",
        "\n",
        "      \n",
        "# Creating a dictionary with unique topics wrt average views\n",
        "unique_topics_avg_view_dict={}\n",
        "for topic in unique_topics:\n",
        "  temp, count = 0, 0\n",
        "\n",
        "  for i in range(0,len(tedtalk_df)):\n",
        "    temp2=tedtalk_df['topics'][i]\n",
        "    if(topic in temp2):\n",
        "      temp+=tedtalk_df['views_per_day'][i]\n",
        "      count+=1\n",
        "  unique_topics_avg_view_dict[topic]=temp//count\n",
        "\n",
        "# Creating a list and appending with average views wrt topic\n",
        "topics_wise_average_views=[]\n",
        "for i in range(0,len(tedtalk_df)):\n",
        "  temp=0\n",
        "  temp_topic=tedtalk_df['topics'][i]\n",
        "  for element in temp_topic:\n",
        "    temp+= unique_topics_avg_view_dict[element]\n",
        "  \n",
        "  topics_wise_average_views.append(temp//len(temp_topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4xEJBQ8ydVJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creating a new columns with average topic views\n",
        "tedtalk_df['topics_wise_avg_views'] = pd.Series(topics_wise_average_views).values\n",
        "\n",
        "# Creating a unique topics column which contain number of unique topics spoken on in an event\n",
        "tedtalk_df['unique_topics'] = tedtalk_df['topics'].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk4Hi8q9yhiz"
      },
      "outputs": [],
      "source": [
        "tedtalk_df['subtitles_count'] = tedtalk_df.available_lang.apply(lambda x: len(x))\n",
        "\n",
        "tedtalk_df['Telecasted_after'] = (tedtalk_df['published_date'] - tedtalk_df['recorded_date']).apply(lambda x: x.days)\n",
        "#creating a new variable 'telecasted after' which holds the difference between published date and recorded date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hazBWRPDyjVB"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEasyRWyyllM"
      },
      "outputs": [],
      "source": [
        "tedtalk_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32NnPvLOyolr"
      },
      "outputs": [],
      "source": [
        "df = tedtalk_df[['comments','duration','release_day','release_month','release_year',\n",
        "                 'Telecasted_after','video_age','views_per_day','speaker_1_average_views','event_average_views','topics_wise_avg_views','unique_topics','subtitles_count']]\n",
        "                 #taking only imp columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNdWJw55yqK4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbBzJE9IysdX"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyX8wB1EyuyR"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quETSsWdalev"
      },
      "source": [
        "Now that morevariables are created in the above steps ,We can check for the ouliers once again to make sure that our data is free of outliers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN-YJucWyxrK"
      },
      "outputs": [],
      "source": [
        "def remove_outlier(df,column):\n",
        "  \n",
        "  plt.figure(figsize=(15,6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('Before Treating outliers')\n",
        "  sns.boxplot(df[column])\n",
        "  plt.subplot(1, 2, 2)\n",
        "  sns.distplot(df[column])\n",
        "  df = df[((df[column] - df[column].mean()) / df[column].std()).abs() < 3]\n",
        "  \n",
        "  plt.figure(figsize=(15,6))\n",
        "  \n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('After Treating outliers')\n",
        "  sns.boxplot(df[column])\n",
        "  plt.subplot(1, 2, 2)\n",
        "  sns.distplot(df[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTgyy84ey0AV"
      },
      "outputs": [],
      "source": [
        "df.describe().columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ZlNqZDy1xy"
      },
      "outputs": [],
      "source": [
        "for column in df.describe().columns:\n",
        "  remove_outlier(df,column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm2W7KLvy7EI"
      },
      "outputs": [],
      "source": [
        "df.drop('Telecasted_after',axis=1,inplace=True)\n",
        "#It has alot of outliers...better not to consider it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdCroMxDy_VR"
      },
      "outputs": [],
      "source": [
        "df_2 = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB3810y9bU4c"
      },
      "source": [
        "#**Checking the conditions for LR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capguAVLbasb"
      },
      "source": [
        "Now that Feature engineering is done, we need to start with train test split and start building our models. But since we are trying to start with linear regression in here, we need to check several criterias for LR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Z366n6bUrd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRU5IfAuzGC_"
      },
      "outputs": [],
      "source": [
        "# defining in X and Y\n",
        "y = df['views_per_day']\n",
        "X = df.drop(columns=['views_per_day'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBywv4phzJTE"
      },
      "outputs": [],
      "source": [
        "X.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUvb3qSxzNSl"
      },
      "outputs": [],
      "source": [
        "#to check multicollinearity, normal distribution, we need only numerical data, we dont want categorical variable\n",
        "X_without_categorical_variable  = X[['comments', 'duration','video_age', 'speaker_1_average_views',\n",
        "                                      'event_average_views', 'topics_wise_avg_views', 'unique_topics','subtitles_count']]\n",
        "\n",
        "# to check linearity with the dependent variable.\n",
        "df_without_categorical_variable =  df[['comments', 'duration','video_age','views_per_day','speaker_1_average_views',\n",
        "                                      'event_average_views', 'topics_wise_avg_views', 'unique_topics','subtitles_count']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxCTeCgabev1"
      },
      "source": [
        "**1) Checking linearity betweeen the dependent and independent variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv1vKj64zQcB"
      },
      "outputs": [],
      "source": [
        "for column in df_without_categorical_variable:\n",
        "  sns.lmplot(x=column, y=\"views_per_day\", data=df_without_categorical_variable, order=1)\n",
        "# plt.ylabel('Target')\n",
        "# plt.xlabel('Independent variable')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCpwSWzb4Si"
      },
      "source": [
        "**2) Multi collinearity should be avoided**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JtItmyBzXn6"
      },
      "outputs": [],
      "source": [
        "DF = X_without_categorical_variable.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WptXY7JuzUpk"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkPXqPHOzbb-"
      },
      "outputs": [],
      "source": [
        "VIF_df = calc_vif(X[[i for i in DF.describe().columns ]])\n",
        "VIF_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vWmrYAeAOrn"
      },
      "outputs": [],
      "source": [
        "calc_vif(DF[[i for i in DF.describe().columns if i not in ['video_age','topics_wise_avg_views','subtitles_count'] ]])\n",
        "#trying to leave which ever had >10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUZYd6tScBHS"
      },
      "source": [
        "By VIF we have kept only few of the columns with less VIF score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPVUTj9acKQs"
      },
      "source": [
        "**3)All independent variables shoould be normally distributed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zou-6iDeca1b"
      },
      "source": [
        "Here we checking from QQ plot "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwT7FEO4chf-"
      },
      "source": [
        "#Applying Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_TxWzv_zhTu"
      },
      "outputs": [],
      "source": [
        "#As we have more number of varibales,trying to create a function which does the plotting and transformation.\n",
        "#But be carefull as we have several variables with positively skewed data and 'not-normal' data.\n",
        "#Whenever we have positively skewed data we need to use the log transformation. Or else we can use Boxcox transformation.\n",
        "\n",
        "#import libraries\n",
        "import scipy.stats as stats\n",
        "import pylab\n",
        "\n",
        "def to_plot(column):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.subplot(1,2,1)\n",
        "  sns.distplot(DF[column])\n",
        "  plt.subplot(1,2,2)\n",
        "  stats.probplot(DF[column],dist='norm',plot=pylab)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def log_transform(column):\n",
        "  print(\"Before Transformation\")\n",
        "  to_plot(column)\n",
        "  # applying log transformation\n",
        "  DF[column]=np.log1p(DF[column])\n",
        "  #plotting\n",
        "  print(\"After Transformation\")\n",
        "  to_plot(column)\n",
        "  # stats.probplot()\n",
        "\n",
        "def box_cox_transform(column):\n",
        "  print(\"Before Transformation\")\n",
        "  to_plot(column)\n",
        "  # applying boxcox transformation\n",
        "  DF[column],parameters=stats.boxcox(DF[column])\n",
        "  print(\"After Transformation\")\n",
        "  to_plot(column)\n",
        "\n",
        "def square_root_transform(column):\n",
        "  print(\"Before Transformation\")\n",
        "  to_plot(column)\n",
        "  DF[column]=DF[column]**(1/2)\n",
        "  print(\"After Transformation\")\n",
        "  to_plot(column)\n",
        "\n",
        "def exponential_transform(column):\n",
        "  print(\"Before Transformation\")\n",
        "  to_plot(column)\n",
        "  DF[column]=DF[column]**(1/1.2)\n",
        "  print(\"After Transformation\")\n",
        "  to_plot(column)\n",
        "\n",
        "\n",
        "def power_transform(column):\n",
        "  print(\"Applying Power Transformation.\\n Before Transformation\")\n",
        "  to_plot(column)\n",
        "  # applying power transformation\n",
        "  DF[column]=np.power(DF[column],1/2)\n",
        "  #plotting\n",
        "  print(\"After Transformation\")\n",
        "  to_plot(column)\n",
        "  # stats.probplot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FWhLbRTzrhT"
      },
      "outputs": [],
      "source": [
        "for column in DF:\n",
        "  to_plot(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lryNGrEmzubv"
      },
      "outputs": [],
      "source": [
        "square_root_transform('comments')\n",
        "box_cox_transform('duration')\n",
        "\n",
        "square_root_transform('speaker_1_average_views')\n",
        "box_cox_transform('event_average_views')\n",
        "\n",
        "square_root_transform('unique_topics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNCdTuOydfJK"
      },
      "source": [
        "After applying the transformation and treating with VIF factor checking again for correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WZxPj3gz0RX"
      },
      "outputs": [],
      "source": [
        "DF.head()\n",
        "#Which does not contain categorical and dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92OCXLwzz2vo"
      },
      "outputs": [],
      "source": [
        "corr = DF.corr()\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gia9nDsNz5bQ"
      },
      "outputs": [],
      "source": [
        "DF.drop('event_average_views',axis=1,inplace=True)\n",
        "#Strong correlation \n",
        "DF.drop('topics_wise_avg_views',axis=1,inplace=True)\n",
        "DF.drop('subtitles_count',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ72wkFIz8I4"
      },
      "outputs": [],
      "source": [
        "corr = DF.corr()\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF2-A4Jsdi5c"
      },
      "source": [
        "We saw that event wise average views a=were highly correlated wih speaker average views, so we tried dropping that out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llfe4PG_z_Y-"
      },
      "outputs": [],
      "source": [
        "p = sns.pairplot(DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aG6hkB1dm2-"
      },
      "source": [
        "Just trying to visualize the DF treating with correlation and transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEuTdz4mdwSi"
      },
      "source": [
        "**One Hot Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcyVtMF3d3Pa"
      },
      "source": [
        "DF contains only the numeric columns, so we merge it with the categorical column. And we use One hot encoding to use the categorical columns in a better way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PHzBTym0CYb"
      },
      "outputs": [],
      "source": [
        "DF.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DlsU-hM0HTy"
      },
      "outputs": [],
      "source": [
        "features_df = DF.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcbIpkR90Jbl"
      },
      "outputs": [],
      "source": [
        "features_df['release_day'] = df['release_day']\n",
        "features_df['release_month'] = df['release_month']\n",
        "features_df['release_year'] = df['release_year']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCcKHSZU0Lgm"
      },
      "outputs": [],
      "source": [
        "features_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygYRGDh20ORh"
      },
      "outputs": [],
      "source": [
        "features_df = pd.get_dummies(features_df, columns=['release_day', 'release_month', 'release_year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xh_RgeQ0Qco"
      },
      "outputs": [],
      "source": [
        "features_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfR4naNygWkP"
      },
      "source": [
        "**Min Max Scaler**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYWmz55Lgf-h"
      },
      "source": [
        "Now everythin is done except normalization, we need to have the dat in same standards to get optimized results for the models. So we use Min max scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C-XR2i40SIq"
      },
      "outputs": [],
      "source": [
        "#Min max scaler\n",
        "\n",
        "\n",
        "column_names = list(features_df.columns)\n",
        "# column_names\n",
        "\n",
        "#taking columns to do the minmaxscaling\n",
        "DF_scaled = pd.DataFrame()\n",
        "#using standardization as both numeric columns are in different scale\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(features_df)\n",
        "#print(scaled)\n",
        "DF_scaled = pd.DataFrame(scaler.fit_transform(features_df))\n",
        "DF_scaled.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t53uSF2r0X8P"
      },
      "outputs": [],
      "source": [
        "\n",
        "DF_scaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6vGsm7x0a0l"
      },
      "outputs": [],
      "source": [
        "DF.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBl7_nNgrAU"
      },
      "source": [
        "**Dependent Variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt1jKUHggwlW"
      },
      "source": [
        "All the normalization and transformations were done only for the independent variables. Now we try doing the same for dependent variable as well to get better result for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE9rAiZ90eV_"
      },
      "outputs": [],
      "source": [
        "df['views_per_day'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOy_lP_O0lDM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.subplot(1,2,1)\n",
        "sns.distplot(df['views_per_day'])\n",
        "plt.subplot(1,2,2)\n",
        "stats.probplot(df['views_per_day'],dist='norm',plot=pylab)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxTTf1Ko0oCa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_(col):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.subplot(1,2,1)\n",
        "  sns.distplot(col)\n",
        "  plt.subplot(1,2,2)\n",
        "  stats.probplot(col,dist='norm',plot=pylab)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_86RxUC_0qQz"
      },
      "outputs": [],
      "source": [
        "stats.probplot(df['views_per_day'],dist='norm',plot=pylab)\n",
        "y1 = np.power(df['views_per_day'],1/3)\n",
        "plot_(y1)\n",
        "y2=np.power(df['views_per_day'],1/2)\n",
        "plot_(y2)\n",
        "y3=np.log1p(df['views_per_day'])\n",
        "plot_(y3)\n",
        "#y4=stats.boxcox(df['views_per_day'])\n",
        "y5=df['views_per_day']**(1/2)\n",
        "plot_(y5)\n",
        "y6=df['views_per_day']**(1/1.2)\n",
        "plot_(y6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZpi-Mc-0uXa"
      },
      "outputs": [],
      "source": [
        "# df['views_per_day'] = np.power(df['views_per_day'],1/3)\n",
        "# plot_(df['views_per_day'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcaHZwjrg6rt"
      },
      "source": [
        "Now our Data is all set to build model .Before that lets just do the train and test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EiIApeihRT_"
      },
      "source": [
        "#Train and Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA0bgHo2hb5P"
      },
      "source": [
        "We have already taken the dependent variable seperately, lets just equate it into y and X has ll the indeoendent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fmik9Hk0x-7"
      },
      "outputs": [],
      "source": [
        "df.head(2)\n",
        "y = y1\n",
        "X = DF_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QsW4KdA00dK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(X, y,  test_size= 0.25, random_state= 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7AlMM5hhuR"
      },
      "source": [
        "# Model Building "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fUgx1xH04Sq"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR\n",
        "import time\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt4NjL0I09rD"
      },
      "outputs": [],
      "source": [
        "def Calculating_errors(y_train,y_pred_train,y_test,y_test_pred):\n",
        "  MSE_test  = mean_squared_error(y_test, y_test_pred)\n",
        "  print(\"MSE on test is\" ,MSE_test)\n",
        "  MSE_train  = mean_squared_error(y_train, y_pred_train)\n",
        "  print(\"MSE on train is\" ,MSE_train)\n",
        "  RMSE_test = np.sqrt(MSE_test)\n",
        "  print(\"RMSE on test is\" ,RMSE_test)\n",
        "  RMSE_train = np.sqrt(MSE_train)\n",
        "  print(\"RMSE on train is\" ,RMSE_train)\n",
        "  print('Training MAE: {:0.2f}'.format(mean_absolute_error(y_train, y_pred_train)))\n",
        "  print('Test MAE: {:0.2f}'.format(mean_absolute_error(y_test, y_test_pred)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke2MJ069hokr"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBx6NMxl1BJD"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNYVRfbO1FUC"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,lin_reg.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,lin_reg.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL80BRSM1IfN"
      },
      "outputs": [],
      "source": [
        "y_pred_train_reg =lin_reg.predict(X_train)\n",
        "y_test_pred_reg = lin_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Nabt3k1Kws"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_pred_train_reg,y_test,y_test_pred_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aESOJmNJhveI"
      },
      "source": [
        "# Regularised linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhEP9xBK1WPn"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh1ovvBx1YmM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression,ElasticNet\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,VotingRegressor,StackingRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8II00oJ1bcH"
      },
      "outputs": [],
      "source": [
        "elastic_param = {'alpha' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "                 'l1_ratio' : [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gbq-mbF1dqX"
      },
      "outputs": [],
      "source": [
        "elastic_grid = GridSearchCV(estimator=ElasticNet(),\n",
        "                       param_grid = elastic_param,n_jobs=9,\n",
        "                       cv = 5, verbose=2, scoring='r2')\n",
        "elastic_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tDKzd2u1k80"
      },
      "outputs": [],
      "source": [
        "optimal_elastic = elastic_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0QXpFqY1qcG"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,optimal_elastic.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,optimal_elastic.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYYS9YeE1tAy"
      },
      "outputs": [],
      "source": [
        "y_pred_train_ela = optimal_elastic.predict(X_train)\n",
        "y_test_pred_ela = optimal_elastic.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlKywuVy1u0t"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_pred_train_ela,y_test,y_test_pred_ela)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsMs9sRyh8QJ"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA2OmxdL1xWD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94UjFK291zEL"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(criterion='mae')\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG3LcCyb12gX"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,rf.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,rf.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6xf4DJ016I5"
      },
      "outputs": [],
      "source": [
        "y_train_pred_rf = rf.predict(X_train)\n",
        "y_test_pred_rf = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2US9WA7A18IA"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_rf,y_test,y_test_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QnHMtPwiDDL"
      },
      "source": [
        "# Optimal Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60n60iWk1_RM"
      },
      "outputs": [],
      "source": [
        "rf_param = {\n",
        "            'n_estimators':[100,250,500],\n",
        "            'max_depth':[2,4,5,6],\n",
        "            'max_leaf_nodes':[150]\n",
        "           }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITgB6E2b2BOP"
      },
      "outputs": [],
      "source": [
        "rf_grid = GridSearchCV(estimator=RandomForestRegressor(),\n",
        "                       param_grid = rf_param,n_jobs=9,\n",
        "                       cv = 5, verbose=2, scoring='r2')\n",
        "rf_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HYPUIMH2FTv"
      },
      "outputs": [],
      "source": [
        "optimal_rf = rf_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjgchdzA2ISg"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,optimal_rf.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,optimal_rf.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zTVlPtT2KO6"
      },
      "outputs": [],
      "source": [
        "y_train_pred_optimal_rf = optimal_rf.predict(X_train)\n",
        "y_test_pred_optimal_rf = optimal_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m22Uxqtv2NId"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_optimal_rf,y_test,y_test_pred_optimal_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20f0OCTliNFE"
      },
      "source": [
        "# CatBoost Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKGssC8M2PLU"
      },
      "outputs": [],
      "source": [
        "cat_reg = CatBoostRegressor()\n",
        "cat_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcNvDy4e2XeN"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,cat_reg.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,cat_reg.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1XbQfqR2ZPW"
      },
      "outputs": [],
      "source": [
        "y_train_pred_cat = cat_reg.predict(X_train)\n",
        "y_test_pred_cat = cat_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W_P4_AA2djN"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_cat,y_test,y_test_pred_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_axQbi3jiWUb"
      },
      "source": [
        "# Optimal catboost regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_0ssMKl2gJs"
      },
      "outputs": [],
      "source": [
        "cat_param = {'depth': [2,4,5],\n",
        "             'learning_rate' : [0.01,0.05,0.1],\n",
        "             'iterations'    : [150,200]\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRKSEz_j2iK5"
      },
      "outputs": [],
      "source": [
        "ct_grid = GridSearchCV(estimator=cat_reg,\n",
        "                       param_grid = cat_param,n_jobs=9,\n",
        "                       cv = 5, verbose=2, scoring='r2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JP5ukfS2qUl"
      },
      "outputs": [],
      "source": [
        "ct_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NIXQt5H2r_Y"
      },
      "outputs": [],
      "source": [
        "optimal_cat = ct_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM14EVDZ2_S5"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,optimal_cat.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,optimal_cat.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8WQrFPw2w0u"
      },
      "outputs": [],
      "source": [
        "y_train_pred_optcat = optimal_cat.predict(X_train)\n",
        "y_test_pred_optcat = optimal_cat.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgJbdDjs2yl8"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_optcat,y_test,y_test_pred_optcat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3GB3DEdiicB"
      },
      "source": [
        "# XG Boost Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YJuJGvh3FIR"
      },
      "outputs": [],
      "source": [
        "xgb_reg = XGBRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exIBVrot3G3r"
      },
      "outputs": [],
      "source": [
        "xgb_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fiIJd973JaB"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,xgb_reg.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,xgb_reg.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HlCb-vV3pA6"
      },
      "outputs": [],
      "source": [
        "y_train_pred_xgb = xgb_reg.predict(X_train)\n",
        "y_test_pred_xgb = xgb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB2ziwap3qwe"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_xgb,y_test,y_test_pred_xgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKo-cHuiiqGD"
      },
      "source": [
        "# Optimised XGboost "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwO__npk3uVZ"
      },
      "outputs": [],
      "source": [
        "xgb_param = {'n_estimators': [50,100,150,200],\n",
        "            'max_depth': [2,4,5,10],\n",
        "            'learning_rate':[0.01,0.05,0.1]\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXuogPRy3wTV"
      },
      "outputs": [],
      "source": [
        "xgb_grid = GridSearchCV(estimator=xgb_reg,\n",
        "                       param_grid = xgb_param,n_jobs=9,\n",
        "                       cv = 5, verbose=2, scoring='r2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMW0Zytx3yDs"
      },
      "outputs": [],
      "source": [
        "xgb_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLUTY5jT30JI"
      },
      "outputs": [],
      "source": [
        "optimal_xgb = xgb_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG0Qq-BL33My"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,optimal_xgb.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,optimal_xgb.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onUtCGPn34zs"
      },
      "outputs": [],
      "source": [
        "y_train_pred_optxgb = optimal_xgb.predict(X_train)\n",
        "y_test_pred_optxgb = optimal_xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgsj42KN37Jr"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_optxgb,y_test,y_test_pred_optxgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm3kNZoiy4Y"
      },
      "source": [
        "# LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Fx2iNu4AQj"
      },
      "outputs": [],
      "source": [
        "lgbm_reg = LGBMRegressor()\n",
        "lgbm_reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ay5QAK4COw"
      },
      "outputs": [],
      "source": [
        "r2_score(y_train,lgbm_reg.predict(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sP3WdxX4ECl"
      },
      "outputs": [],
      "source": [
        "r2_score(y_test,lgbm_reg.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IzmD_dR4Gcj"
      },
      "outputs": [],
      "source": [
        "y_train_pred_lgbm = lgbm_reg.predict(X_train)\n",
        "y_test_pred_lgbm = lgbm_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_l1dMhy4IC0"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_lgbm,y_test,y_test_pred_lgbm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQ6BMgti3zx"
      },
      "source": [
        "# Optimal LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVRqdrj_4LNF"
      },
      "outputs": [],
      "source": [
        "lgbm_param = {\n",
        "              \"n_estimators\" :[50,100,150],\n",
        "              'num_leaves': [6,8,12,16],\n",
        "              \"max_depth\": [2,4,5],\n",
        "              \"learning_rate\": [0.01,0.05,0.1]\n",
        "              }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwjts6q04Mwr"
      },
      "outputs": [],
      "source": [
        "lgbm_grid = GridSearchCV(estimator=lgbm_reg,\n",
        "                       param_grid = lgbm_param,n_jobs=6,\n",
        "                       cv = 5, verbose=2, scoring='r2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91a4bGhw4PKx"
      },
      "outputs": [],
      "source": [
        "lgbm_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfbOLVRM4R0h"
      },
      "outputs": [],
      "source": [
        "lgbm_optimal = lgbm_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcI4pUe24TWA"
      },
      "outputs": [],
      "source": [
        "r2_score(y_test,lgbm_optimal.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMKJJeSz4Vxq"
      },
      "outputs": [],
      "source": [
        "r2_score(y_train,lgbm_optimal.predict(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFkHIZoo4Yb-"
      },
      "outputs": [],
      "source": [
        "y_train_pred_lgbm_optimal = lgbm_optimal.predict(X_train)\n",
        "y_test_pred_lgbm_optimal = lgbm_optimal.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WggZGPYg4a-M"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_lgbm_optimal,y_test,y_test_pred_lgbm_optimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOBS64HajFqo"
      },
      "source": [
        "# Extra Tree Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD6t3DYY4efv"
      },
      "outputs": [],
      "source": [
        "extra_tree_reg = ExtraTreesRegressor(criterion='mae', max_depth=30, n_estimators=200, min_samples_leaf=2, min_samples_split=6)\n",
        "extra_tree_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1venOU1Z4hE8"
      },
      "outputs": [],
      "source": [
        "print(f\"The r2 score for testing is {round(r2_score(y_test,extra_tree_reg.predict(X_test)),4)}\")\n",
        "print(f\"The r2 score for training is {round(r2_score(y_train,extra_tree_reg.predict(X_train)),4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TZedK-J4jwi"
      },
      "outputs": [],
      "source": [
        "y_train_pred_extra = extra_tree_reg.predict(X_train)\n",
        "y_test_pred_extra = extra_tree_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylwfJCqy4mA5"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_extra,y_test,y_test_pred_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKkEr1tVjM_X"
      },
      "source": [
        "# Optimal Extra Tree Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up-uxwY64p8j"
      },
      "outputs": [],
      "source": [
        "#Extra Trees Regressor paramdict\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "# HYperparameter Dict\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CqcPaLFA8Pgp"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Extratrees regressor\n",
        "et_model = ExtraTreesRegressor(criterion='mae')\n",
        "\n",
        "#RandomSearch\n",
        "et_random = RandomizedSearchCV(et_model,param_dict,verbose=2,cv=5)\n",
        "et_random.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EWjCsD864ulK"
      },
      "outputs": [],
      "source": [
        "et_optimal_model = et_random.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "69TyMVg_4wUj"
      },
      "outputs": [],
      "source": [
        "y_train_pred_etopt = et_optimal_model.predict(X_train)\n",
        "y_test_pred_etopt = et_optimal_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zd9g3AP44ynN"
      },
      "outputs": [],
      "source": [
        "Calculating_errors(y_train,y_train_pred_etopt,y_test,y_test_pred_etopt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "06BqHmIe43dE"
      },
      "outputs": [],
      "source": [
        "# make predictions for test data\n",
        "y_hat = et_optimal_model.predict(X_test)\n",
        "print(f'r_sqr value for train: {et_optimal_model.score(X_train, y_train)}')\n",
        "r_squared= r2_score(y_test,y_hat)\n",
        "#Calculate Adjusted R-sqaured\n",
        "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "print(f'For test the R_Squared for ExtraTreesRegressor is {r_squared} and adjusted R_Squared is {adjusted_r_squared}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQ4Qw_Ljb6_"
      },
      "source": [
        "**Comparing all models with model performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dco59ni_47M6"
      },
      "outputs": [],
      "source": [
        "regressors = [optimal_elastic,optimal_rf,optimal_cat,xgb_reg,lgbm_optimal,et_optimal_model]\n",
        "reg_names = ['Regularized Linear regression','RandomForest','CatBoost','XGBoost','LGBM','Extra tree regressor']\n",
        "training,testing = [],[]\n",
        "for i in regressors:\n",
        "    tr = round(r2_score(y_train,i.predict(X_train)),3)\n",
        "    ts = round(r2_score(y_test,i.predict(X_test)),3)\n",
        "    training.append(tr)\n",
        "    testing.append(ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3JH39WwD4-eQ"
      },
      "outputs": [],
      "source": [
        "diff = np.array(training)-np.array(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m_qxCVwG5Beh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(range(0,len(regressors)),training,'--o',lw=2,label='Training')\n",
        "plt.plot(range(0,len(regressors)),testing,'-o',lw=2,label='Testing')\n",
        "plt.xticks(range(0,len(regressors)), reg_names, rotation=45,fontsize=14)\n",
        "plt.axvline(np.argmin(diff),linestyle=':', color='black', label=f'Best performing model')\n",
        "plt.ylabel(\"Scores\")\n",
        "plt.title(\"Comparing training and testing r2-scores for our models\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='best',fontsize=12);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6nOz1KOjyDM"
      },
      "source": [
        "We can observe that our modt models are overfitting, they performed alot better in traing but their performance is poor in testing. But comparitvely Regularized linear regression is performing well in traing and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvwEntu_j1IK"
      },
      "source": [
        "**Evaluation metric for all models (in comaprision)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2XWdG9on5DbL"
      },
      "outputs": [],
      "source": [
        "#Defining all the models\n",
        "models = [\n",
        "           ['Regularized Linear Regression', elastic_grid.best_estimator_],\n",
        "           ['Optimal Random Forest ', rf_grid.best_estimator_],\n",
        "           ['LGBM ', lgbm_grid.best_estimator_],\n",
        "           ['Catboost ',  ct_grid.best_estimator_],\n",
        "           ['XGBoost ',xgb_reg],    \n",
        "           ['Extra tree regressor ', et_random.best_estimator_]          \n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ca72Nlkz5Ji_"
      },
      "outputs": [],
      "source": [
        "#Let's run all the models and store the scores\n",
        "model_1_data = []\n",
        "for name,model in models :\n",
        "    model_data = {}\n",
        "    model.random_state = 42\n",
        "    model_data[\"Name\"] = name\n",
        "    model.fit(X_train,y_train)\n",
        "    model_data[\"MAE_train\"] = round(metrics.mean_absolute_error(y_train, model.predict(X_train)),4)\n",
        "    model_data[\"MAE_test\"] = round(metrics.mean_absolute_error(y_test, model.predict(X_test)),4)\n",
        "    model_data[\"MSE_train\"] = round(metrics.mean_squared_error(y_train, model.predict(X_train)),4)\n",
        "    model_data[\"MSE_test\"] = round(metrics.mean_squared_error(y_test, model.predict(X_test)),4)\n",
        "    model_data[\"R2_Score_train\"] = round(r2_score(y_train,model.predict(X_train)),4)\n",
        "    model_data[\"R2_Score_test\"] = round(r2_score(y_test,model.predict(X_test)),4)\n",
        "    model_data['Adjusted_R2_score_train'] = 1 - (1-(r2_score(y_train,model.predict(X_train))))*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "    model_data['Adjusted_R2_score_test'] = 1 - (1-(r2_score(y_test,model.predict(X_test))))*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "    model_data[\"RMSE_Score_train\"] = round(np.sqrt(mean_squared_error(y_train,model.predict(X_train))),4)\n",
        "    model_data[\"RMSE_Score_test\"] = round(np.sqrt(mean_squared_error(y_test,model.predict(X_test))),4)\n",
        "    model_1_data.append(model_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZn8w_JP5L01"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(model_1_data)\n",
        "results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oJCVm5HkDBE"
      },
      "source": [
        "# Conclusion \n",
        "\n",
        "That's it! We reached the end of our exercise.\n",
        "\n",
        "Started with loading the data so far we have done EDA ,feature engineering , data cleaning, target encoding and one hot encoding of categorical columns, feature selection and then model building.\n",
        "\n",
        "**So far we have modelled on**\n",
        "\n",
        "Lasso Regressor\n",
        "\n",
        "Ridge Regressor\n",
        "\n",
        "\n",
        "Random Forest Regressor\n",
        "\n",
        "Extra Tree Regressor\n",
        "\n",
        "Gradient Boosting Regressor\n",
        "\n",
        "XGB Regressor\n",
        "\n",
        "In all the features speaker_wise_avg_views is most important this implies that speakers are directly impacting the views.\n",
        "\n",
        "\n",
        "After comparing all the models we can conclude that Optimal Random Forest and aswell as Extra Tree is the best performer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FIxNuOI0QPJ7"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNBFA6v8Cs7Rep8lUth8I9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}